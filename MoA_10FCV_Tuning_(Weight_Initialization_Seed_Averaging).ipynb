{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MoA_10FCV_Tuning (Weight Initialization Seed Averaging)",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPHlPOmmvSmUAKr/JySm7nU"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBLwMVfSgH2N",
        "outputId": "a007f8a0-ceae-4d75-d4c6-aefda6284ecc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "!pip install iterative-stratification"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: iterative-stratification in /usr/local/lib/python3.6/dist-packages (0.1.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->iterative-stratification) (0.17.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-gYJi82kr_4",
        "outputId": "7021ccac-8b4c-4426-de66-defd02ede387",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWfWxZ1miP06"
      },
      "source": [
        "# Library Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_p_Xg6cj3L_"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader as DL\n",
        "from torch.nn.utils import weight_norm as WN\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from time import time\n",
        "import random as r\n",
        "\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m481JkTYiTOD"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iF4DfyCWkrd6"
      },
      "source": [
        "def breaker():\n",
        "  print(\"\\n\" + 30*\"-\" + \"\\n\")\n",
        "\n",
        "def head(x, no_of_ele=5):\n",
        "    breaker()\n",
        "    print(x[:no_of_ele])\n",
        "    breaker()\n",
        "\n",
        "def preprocess(x):\n",
        "  df = x.copy()\n",
        "  df.iloc[:, 1] = df.iloc[:, 1].map({\"trt_cp\" : 0, \"ctl_vehicle\" : 1})\n",
        "  df.iloc[:, 3] = df.iloc[:, 3].map({\"D1\" : 0, \"D2\" : 1})\n",
        "  return df\n",
        "\n",
        "def log_loss_metric(y_true, y_pred, num_classes=206):\n",
        "  metric = []\n",
        "  for i in range(num_classes):\n",
        "    metric.append(log_loss(y_true[:, i], y_pred[:, i], labels=[0, 1], eps=1e-15))\n",
        "  return sum(metric)/num_classes\n",
        "\n",
        "datapath = \"/content/gdrive/My Drive/Datasets/MoA/\"\n",
        "# Try using log_loss_metric as loss to be optimized."
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNDSNZyqiVl0"
      },
      "source": [
        "# Data Handling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AE1I4aPqiXXf"
      },
      "source": [
        "**Input**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIWeK8TUl4Ww",
        "outputId": "376141d9-af88-464d-d42b-9a40ba3accd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        }
      },
      "source": [
        "tr_feat = pd.read_csv(\"/content/gdrive/My Drive/Datasets/MoA/train.csv\")\n",
        "tr_lbls = pd.read_csv(\"/content/gdrive/My Drive/Datasets/MoA/train_targets_s.csv\")\n",
        "tr_lbls_ns = pd.read_csv(\"/content/gdrive/My Drive/Datasets/MoA/train_targets_ns.csv\")\n",
        "\n",
        "breaker()\n",
        "print(tr_feat.shape)\n",
        "breaker()\n",
        "print(tr_lbls.shape)\n",
        "breaker()\n",
        "print(tr_lbls_ns.shape)\n",
        "breaker()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "------------------------------\n",
            "\n",
            "(23814, 876)\n",
            "\n",
            "------------------------------\n",
            "\n",
            "(23814, 207)\n",
            "\n",
            "------------------------------\n",
            "\n",
            "(23814, 403)\n",
            "\n",
            "------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2JopEkriZSA"
      },
      "source": [
        "**Top Features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1_lLkpFmeMP"
      },
      "source": [
        "top_feats = [  1,   2,   3,   4,   5,   6,   7,   9,  11,  14,  15,  16,  17,\n",
        "              18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  29,  30,  31,\n",
        "              32,  33,  35,  36,  37,  38,  39,  40,  41,  42,  43,  44,  46,\n",
        "              47,  48,  49,  50,  51,  52,  53,  54,  55,  56,  58,  59,  60,\n",
        "              61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  71,  72,  73,\n",
        "              74,  75,  76,  78,  79,  80,  81,  82,  83,  84,  86,  87,  88,\n",
        "              89,  90,  91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101,\n",
        "              102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114,\n",
        "              115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128,\n",
        "              129, 130, 131, 132, 133, 136, 137, 138, 139, 140, 141, 142, 143,\n",
        "              144, 145, 146, 147, 149, 150, 151, 152, 153, 154, 155, 156, 157,\n",
        "              158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170,\n",
        "              171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
        "              184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197,\n",
        "              198, 199, 200, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212,\n",
        "              213, 214, 215, 216, 217, 218, 219, 220, 221, 223, 224, 225, 226,\n",
        "              227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,\n",
        "              240, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n",
        "              254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266,\n",
        "              267, 268, 269, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280,\n",
        "              281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 294,\n",
        "              295, 296, 298, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309,\n",
        "              310, 311, 312, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323,\n",
        "              324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336,\n",
        "              337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
        "              350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362,\n",
        "              363, 364, 365, 366, 367, 368, 369, 370, 371, 374, 375, 376, 377,\n",
        "              378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 390, 391,\n",
        "              392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404,\n",
        "              405, 406, 407, 408, 409, 411, 412, 413, 414, 415, 416, 417, 418,\n",
        "              419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431,\n",
        "              432, 434, 435, 436, 437, 438, 439, 440, 442, 443, 444, 445, 446,\n",
        "              447, 448, 449, 450, 453, 454, 456, 457, 458, 459, 460, 461, 462,\n",
        "              463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n",
        "              476, 477, 478, 479, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n",
        "              490, 491, 492, 493, 494, 495, 496, 498, 500, 501, 502, 503, 505,\n",
        "              506, 507, 509, 510, 511, 512, 513, 514, 515, 518, 519, 520, 521,\n",
        "              522, 523, 524, 525, 526, 527, 528, 530, 531, 532, 534, 535, 536,\n",
        "              538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 549, 550, 551,\n",
        "              552, 554, 557, 559, 560, 561, 562, 565, 566, 567, 568, 569, 570,\n",
        "              571, 572, 573, 574, 575, 577, 578, 580, 581, 582, 583, 584, 585,\n",
        "              586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 599,\n",
        "              600, 601, 602, 606, 607, 608, 609, 611, 612, 613, 615, 616, 617,\n",
        "              618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630,\n",
        "              631, 632, 633, 634, 635, 636, 637, 638, 639, 641, 642, 643, 644,\n",
        "              645, 646, 647, 648, 649, 650, 651, 652, 654, 655, 656, 658, 659,\n",
        "              660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672,\n",
        "              673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685,\n",
        "              686, 687, 688, 689, 691, 692, 693, 694, 695, 696, 697, 699, 700,\n",
        "              701, 702, 704, 705, 707, 708, 709, 710, 711, 713, 714, 716, 717,\n",
        "              718, 720, 721, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732,\n",
        "              733, 734, 735, 737, 738, 739, 740, 742, 743, 744, 745, 746, 747,\n",
        "              748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 759, 760, 761,\n",
        "              762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774,\n",
        "              775, 776, 777, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788,\n",
        "              789, 790, 792, 793, 794, 795, 796, 797, 798, 800, 801, 802, 803,\n",
        "              804, 805, 806, 808, 809, 811, 813, 814, 815, 816, 817, 818, 819,\n",
        "              821, 822, 823, 825, 826, 827, 828, 829, 830, 831, 832, 834, 835,\n",
        "              837, 838, 839, 840, 841, 842, 845, 846, 847, 848, 850, 851, 852,\n",
        "              854, 855, 856, 858, 859, 860, 861, 862, 864, 866, 867, 868, 869,\n",
        "              870, 871, 872, 873, 874]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TcjgaWmic0r"
      },
      "source": [
        "**Pre-processing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDlls3Ucn4uJ",
        "outputId": "82cdbbac-d97a-445c-8bf2-0459679f1a56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "features = tr_feat.copy()\n",
        "labels   = tr_lbls.copy()\n",
        "\n",
        "features = preprocess(features)\n",
        "\n",
        "labels   = labels.loc[features[\"cp_type\"] == 0].reset_index(drop=True)\n",
        "features = features.loc[features[\"cp_type\"] == 0].reset_index(drop=True)\n",
        "\n",
        "breaker()\n",
        "print(\"Features Matrix Shape :\", repr(features.shape))\n",
        "breaker()\n",
        "print(\"Labels Matrix Shape   :\", repr(labels.shape))\n",
        "breaker()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "------------------------------\n",
            "\n",
            "Features Matrix Shape : (21948, 876)\n",
            "\n",
            "------------------------------\n",
            "\n",
            "Labels Matrix Shape   : (21948, 207)\n",
            "\n",
            "------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdW15u1Pifxd"
      },
      "source": [
        "**Creating Train and Test Sets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jP10kXE2ocY5"
      },
      "source": [
        "X, X_test, y, y_test = train_test_split(features, labels, test_size=1948, shuffle=True, random_state=0)\n",
        "\n",
        "X, X_test, y, y_test = X.values, X_test.values, y.values, y_test.values\n",
        "\n",
        "X, X_test, y, y_test = X[:, top_feats], X_test[:, top_feats], y[:, 1:], y_test[:, 1:]\n",
        "\n",
        "X, X_test, y, y_test = X.astype(float), X_test.astype(float), y.astype(float), y_test.astype(float)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zW-albhqqOuf"
      },
      "source": [
        "num_features = X.shape[1]\n",
        "num_classes  = y.shape[1]\n",
        "num_obs_test = y_test.shape[0]\n",
        "\n",
        "del tr_feat, tr_lbls"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpEkR-lRilL8"
      },
      "source": [
        "**Dataset Template**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSU3GxODqtRZ"
      },
      "source": [
        "class DS(Dataset):\n",
        "  def __init__(this, X=None, y=None, mode=\"train\"):\n",
        "    this.mode = mode\n",
        "    this.X = X\n",
        "    if mode == \"train\":\n",
        "      this.y = y\n",
        "  \n",
        "  def __len__(this):\n",
        "    return this.X.shape[0]\n",
        "\n",
        "  def __getitem__(this, idx):\n",
        "    if this.mode == \"train\":\n",
        "      return torch.FloatTensor(this.X[idx]), torch.FloatTensor(this.y[idx])\n",
        "    else:\n",
        "      return torch.FloatTensor(this.X[idx])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmuXrv_4ioC6"
      },
      "source": [
        "# ANN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WOSs7MviqJ3"
      },
      "source": [
        "**Config**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHlZnWMSpZK2"
      },
      "source": [
        "class CFG():\n",
        "  tr_batch_size = 128\n",
        "  va_batch_size = 128\n",
        "  ts_batch_size = 128\n",
        "\n",
        "  epochs = 50\n",
        "\n",
        "  IL = num_features\n",
        "  # Baseline Parameters\n",
        "  #HL_1 = [2048, 1024]\n",
        "  #HL_2 = [512, 512, 512]\n",
        "\n",
        "  HL_1 = [2048, 1024]\n",
        "  HL_2 = [2048, 1024, 512]\n",
        "  OL = num_classes\n",
        "  n_folds = 10\n",
        "  n_seeds = 3\n",
        "\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "cfg = CFG()\n",
        "\n",
        "ts_data_setup = DS(X_test, None, \"test\")\n",
        "ts_data = DL(ts_data_setup, batch_size=cfg.ts_batch_size, shuffle=False)\n",
        "\n",
        "metrics = []"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K970lGcRisH8"
      },
      "source": [
        "**Setup**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgvDo6B7pd3k"
      },
      "source": [
        "class ANN(nn.Module):\n",
        "  def __init__(this, IL=None, HL=None, OL=None):\n",
        "    super(ANN, this).__init__()\n",
        "\n",
        "    this.HL = HL\n",
        "    this.DP1 = nn.Dropout(p=0.2)\n",
        "    this.DP2 = nn.Dropout(p=0.5)\n",
        "\n",
        "    if len(HL) == 2:\n",
        "      this.BN1 = nn.BatchNorm1d(IL)\n",
        "      this.FC1 = WN(nn.Linear(IL, HL[0]))\n",
        "\n",
        "      this.BN2 = nn.BatchNorm1d(HL[0])\n",
        "      this.FC2 = WN(nn.Linear(HL[0], HL[1]))\n",
        "\n",
        "      this.BN3 = nn.BatchNorm1d(HL[1])\n",
        "      this.FC3 = WN(nn.Linear(HL[1], OL))\n",
        "    elif len(HL) == 3:\n",
        "      this.BN1 = nn.BatchNorm1d(IL)\n",
        "      this.FC1 = WN(nn.Linear(IL, HL[0]))\n",
        "\n",
        "      this.BN2 = nn.BatchNorm1d(HL[0])\n",
        "      this.FC2 = WN(nn.Linear(HL[0], HL[1]))\n",
        "\n",
        "      this.BN3 = nn.BatchNorm1d(HL[1])\n",
        "      this.FC3 = WN(nn.Linear(HL[1], HL[2]))\n",
        "\n",
        "      this.BN4 = nn.BatchNorm1d(HL[2])\n",
        "      this.FC4 = WN(nn.Linear(HL[2], OL))\n",
        "    else:\n",
        "      raise NotImplementedError(\"Only Allows Networks of Depth 2 or 3\")\n",
        "\n",
        "  def getOptimizer(this):\n",
        "    return optim.Adam(this.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "    \n",
        "  def forward(this, x):\n",
        "    if len(this.HL) == 2:\n",
        "      x = this.BN1(x)\n",
        "      x = this.DP1(x)\n",
        "      x = F.relu(this.FC1(x))\n",
        "      x = this.BN2(x)\n",
        "      x = this.DP2(x)\n",
        "      x = F.relu(this.FC2(x))\n",
        "      x = this.BN3(x)\n",
        "      x = this.DP2(x)\n",
        "      x = torch.sigmoid(this.FC3(x))\n",
        "      return x\n",
        "    else:\n",
        "      x = this.BN1(x)\n",
        "      x = this.DP1(x)\n",
        "      x = F.relu(this.FC1(x))\n",
        "      x = this.BN2(x)\n",
        "      x = this.DP2(x)\n",
        "      x = F.relu(this.FC2(x))\n",
        "      x = this.BN3(x)\n",
        "      x = this.DP2(x)\n",
        "      x = F.relu(this.FC3(x))\n",
        "      x = this.BN4(x)\n",
        "      x = this.DP2(x)\n",
        "      x = torch.sigmoid(this.FC4(x))\n",
        "      return x"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7Zub-V2iugc"
      },
      "source": [
        "**ANN Helpers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsYkdkoqs50V"
      },
      "source": [
        "def train_fn(X=None, y=None, n_folds=None, n_seeds=None, HL_to_use=None):\n",
        "  breaker()\n",
        "  print(\"Training ...\")\n",
        "  breaker()\n",
        "\n",
        "  LP = []\n",
        "  names = []\n",
        "  bestLoss = {\"train\" : np.inf, \"valid\" : np.inf}\n",
        "\n",
        "  r.seed(1729)\n",
        "  seeders = [r.randint(0,99) for i in range(n_seeds)]\n",
        "  start_time = time()\n",
        "\n",
        "  for seed in seeders:\n",
        "    fold = 0\n",
        "    #breaker()\n",
        "    for tr_idx, va_idx in MultilabelStratifiedKFold(n_splits=n_folds, shuffle=True, random_state=0).split(X, y):\n",
        "      print(\"Processing Seed {seed}, Fold {fold} ...\".format(seed=seed, fold=fold+1))\n",
        "\n",
        "      X_train, X_valid, y_train, y_valid = X[tr_idx], X[va_idx], y[tr_idx], y[va_idx]\n",
        "\n",
        "      tr_data_setup = DS(X_train, y_train)\n",
        "      va_data_setup = DS(X_valid, y_valid)\n",
        "\n",
        "      dataloaders = {\"train\" : DL(tr_data_setup, batch_size=cfg.tr_batch_size, shuffle=True, generator=torch.manual_seed(0)),\n",
        "                    \"valid\" : DL(va_data_setup, batch_size=cfg.va_batch_size, shuffle=False)\n",
        "                    }\n",
        "\n",
        "      torch.manual_seed(seed)\n",
        "      model = ANN(cfg.IL, HL_to_use, cfg.OL)\n",
        "      model.to(cfg.device)\n",
        "\n",
        "      optimizer = model.getOptimizer()\n",
        "      scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, eps=1e-6, verbose=True)\n",
        "\n",
        "      for e in range(cfg.epochs):\n",
        "        epochLoss = {\"train\" : 0, \"valid\" : 0}\n",
        "        for phase in [\"train\", \"valid\"]:\n",
        "          if phase == \"train\":\n",
        "            model.train()\n",
        "          else:\n",
        "            model.eval()\n",
        "          lossPerPass = 0\n",
        "\n",
        "          for feats, label in dataloaders[phase]:\n",
        "            feats, label = feats.to(cfg.device), label.to(cfg.device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            with torch.set_grad_enabled(phase == \"train\"):\n",
        "              output = model(feats)\n",
        "              loss   = nn.BCELoss()(output, label)\n",
        "              if phase == \"train\":\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "            lossPerPass = (loss.item()/label.shape[0])\n",
        "          epochLoss[phase] = lossPerPass\n",
        "        LP.append(epochLoss)\n",
        "        scheduler.step(epochLoss[\"valid\"])\n",
        "        name = \"Model_{id}_Fold_{fold}_Seed_{seed}.pt\".format(id=len(HL_to_use), fold=fold, seed=seed)\n",
        "        names.append(name)\n",
        "        torch.save(model.state_dict(), datapath+name)\n",
        "        if epochLoss[\"valid\"] < bestLoss[\"valid\"]:\n",
        "          bestLoss = epochLoss\n",
        "          #name = \"Model_{id}_Fold_{fold}_Seed_{seed}.pt\".format(id=len(HL_to_use), fold=fold, seed=seed)\n",
        "          #names.append(name)\n",
        "          #torch.save(model.state_dict(), datapath+name)\n",
        "      fold += 1\n",
        "\n",
        "  breaker()\n",
        "  print(\"Time Taken to Train {n} folds for {e} epochs : {:.2f} minutes\".format((time()-start_time)/60, n=n_folds, e=cfg.epochs))\n",
        "  breaker()\n",
        "  print(\"Best Loss :\", repr(bestLoss))\n",
        "  breaker()\n",
        "  print(\"Training Completed\")\n",
        "  breaker()\n",
        "    \n",
        "  return LP, names, model\n",
        "\n",
        "def eval_fn(model=None, names=None, dataloader=None):\n",
        "  y_pred = np.zeros((num_obs_test, num_classes))\n",
        "\n",
        "  for name in names:\n",
        "    Pred = torch.zeros(cfg.ts_batch_size, num_classes).to(cfg.device)\n",
        "    model.load_state_dict(torch.load(datapath+name))\n",
        "    model.eval()\n",
        "    for feat in dataloader:\n",
        "      feat = feat.to(cfg.device)\n",
        "      with torch.no_grad():\n",
        "        Prob = model(feat)\n",
        "      Pred = torch.cat((Pred, Prob), dim=0)\n",
        "    Pred = Pred[cfg.ts_batch_size:]\n",
        "    Pred = Pred.cpu().numpy()\n",
        "    y_pred = np.add(y_pred, Pred)\n",
        "  y_pred = np.divide(y_pred, len(names))\n",
        "  return y_pred"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U17CVDMPulvG"
      },
      "source": [
        "# 7-Fold Cross Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiD7upbdun9u"
      },
      "source": [
        "**Configuration 1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxJGl2IrurBn",
        "outputId": "5187b587-9713-4cd5-982e-edcbd8488beb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "LP_1, names_1, Network_1 = train_fn(X=X, y=y, n_folds=cfg.n_folds, n_seeds=cfg.n_seeds, HL_to_use=cfg.HL_1)\n",
        "\n",
        "y_pred_1 = eval_fn(Network_1, names_1, ts_data)\n",
        "\n",
        "breaker()\n",
        "print(\"Log Loss : {:.5f}\".format(log_loss_metric(y_test, y_pred_1)))\n",
        "breaker()\n",
        "\n",
        "metrics.append(log_loss_metric(y_test, y_pred_1))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "------------------------------\n",
            "\n",
            "Training ...\n",
            "\n",
            "------------------------------\n",
            "\n",
            "Processing Seed 83, Fold 1 ...\n",
            "Epoch    17: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    27: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    31: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Processing Seed 83, Fold 2 ...\n",
            "Epoch    22: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    32: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    40: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Processing Seed 83, Fold 3 ...\n",
            "Epoch    22: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    26: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    30: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Processing Seed 83, Fold 4 ...\n",
            "Epoch    21: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    27: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    31: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Processing Seed 83, Fold 5 ...\n",
            "Epoch    24: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    36: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    40: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Processing Seed 83, Fold 6 ...\n",
            "Epoch    13: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    47: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Processing Seed 83, Fold 7 ...\n",
            "Epoch    15: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    28: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Processing Seed 83, Fold 8 ...\n",
            "Epoch    26: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    35: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    39: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Processing Seed 83, Fold 9 ...\n",
            "Epoch    27: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    38: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    42: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Processing Seed 83, Fold 10 ...\n",
            "Epoch    21: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    25: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    31: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Processing Seed 5, Fold 1 ...\n",
            "Epoch    17: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    21: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    27: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Processing Seed 5, Fold 2 ...\n",
            "Epoch    24: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    29: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    33: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Processing Seed 5, Fold 3 ...\n",
            "Epoch    20: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    28: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    32: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Processing Seed 5, Fold 4 ...\n",
            "Epoch    20: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    29: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    38: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Processing Seed 5, Fold 5 ...\n",
            "Epoch    27: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    41: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    45: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Processing Seed 5, Fold 6 ...\n",
            "Epoch    21: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    28: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    32: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Processing Seed 5, Fold 7 ...\n",
            "Epoch    27: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    39: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    46: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Processing Seed 5, Fold 8 ...\n",
            "Epoch    29: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    33: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    37: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Processing Seed 5, Fold 9 ...\n",
            "Epoch    25: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    35: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    39: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Processing Seed 5, Fold 10 ...\n",
            "Epoch    24: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    28: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    32: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Processing Seed 52, Fold 1 ...\n",
            "Epoch    18: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    26: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    30: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Processing Seed 52, Fold 2 ...\n",
            "Epoch    25: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    29: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    33: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Processing Seed 52, Fold 3 ...\n",
            "Epoch    24: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    28: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    37: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Processing Seed 52, Fold 4 ...\n",
            "Epoch    22: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    26: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    30: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Processing Seed 52, Fold 5 ...\n",
            "Epoch    28: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    43: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    47: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Processing Seed 52, Fold 6 ...\n",
            "Epoch    19: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    28: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    32: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Processing Seed 52, Fold 7 ...\n",
            "Epoch     9: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    35: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    41: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Processing Seed 52, Fold 8 ...\n",
            "Epoch    19: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    30: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Processing Seed 52, Fold 9 ...\n",
            "Epoch    19: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    28: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    33: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Processing Seed 52, Fold 10 ...\n",
            "Epoch    17: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    28: reducing learning rate of group 0 to 1.0000e-06.\n",
            "\n",
            "------------------------------\n",
            "\n",
            "Time Taken to Train 10 folds for 50 epochs : 25.42 minutes\n",
            "\n",
            "------------------------------\n",
            "\n",
            "Best Loss : {'train': 0.0001511920359916985, 'valid': 0.00016779606230556964}\n",
            "\n",
            "------------------------------\n",
            "\n",
            "Training Completed\n",
            "\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "\n",
            "Log Loss : 0.01651\n",
            "\n",
            "------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JTUu7LButkU"
      },
      "source": [
        "**Configuration 2**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGbe9xbPus7e",
        "outputId": "1e96343f-81b9-4d38-f8bf-fc27fdebfb91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "LP_2, names_2, Network_2 = train_fn(X=X, y=y, n_seeds=cfg.n_seeds, n_folds=cfg.n_folds, HL_to_use=cfg.HL_2)\n",
        "\n",
        "y_pred_2 = eval_fn(Network_2, names_2, ts_data)\n",
        "\n",
        "breaker()\n",
        "print(\"Log Loss : {:.5f}\".format(log_loss_metric(y_test, y_pred_2)))\n",
        "breaker()\n",
        "\n",
        "metrics.append(log_loss_metric(y_test, y_pred_2))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "------------------------------\n",
            "\n",
            "Training ...\n",
            "\n",
            "------------------------------\n",
            "\n",
            "Processing Seed 83, Fold 1 ...\n",
            "Epoch    30: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    34: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    38: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Processing Seed 83, Fold 2 ...\n",
            "Epoch    28: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    45: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Processing Seed 83, Fold 3 ...\n",
            "Epoch    21: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    38: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    46: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Processing Seed 83, Fold 4 ...\n",
            "Epoch    26: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    35: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    41: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Processing Seed 83, Fold 5 ...\n",
            "Epoch    27: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    44: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Processing Seed 83, Fold 6 ...\n",
            "Epoch    28: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    35: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    39: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Processing Seed 83, Fold 7 ...\n",
            "Epoch    20: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    40: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    44: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Processing Seed 83, Fold 8 ...\n",
            "Epoch    38: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    46: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    50: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Processing Seed 83, Fold 9 ...\n",
            "Epoch    25: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    41: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    45: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Processing Seed 83, Fold 10 ...\n",
            "Epoch    31: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    41: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    45: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Processing Seed 5, Fold 1 ...\n",
            "Epoch    28: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    34: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    38: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Processing Seed 5, Fold 2 ...\n",
            "Epoch    26: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    38: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    46: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Processing Seed 5, Fold 3 ...\n",
            "Epoch    20: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    27: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    35: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Processing Seed 5, Fold 4 ...\n",
            "Epoch    25: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    42: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Processing Seed 5, Fold 5 ...\n",
            "Epoch    37: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Processing Seed 5, Fold 6 ...\n",
            "Epoch    26: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    43: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    47: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Processing Seed 5, Fold 7 ...\n",
            "Epoch    37: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Processing Seed 5, Fold 8 ...\n",
            "Epoch    38: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    44: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    48: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Processing Seed 5, Fold 9 ...\n",
            "Epoch    24: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    30: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Processing Seed 5, Fold 10 ...\n",
            "Epoch    28: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    36: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    40: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Processing Seed 52, Fold 1 ...\n",
            "Epoch    23: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    41: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    45: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Processing Seed 52, Fold 2 ...\n",
            "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    42: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    48: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Processing Seed 52, Fold 3 ...\n",
            "Epoch    25: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    34: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    41: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Processing Seed 52, Fold 4 ...\n",
            "Epoch    30: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    40: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    44: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Processing Seed 52, Fold 5 ...\n",
            "Epoch    28: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Processing Seed 52, Fold 6 ...\n",
            "Epoch    20: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    35: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    39: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Processing Seed 52, Fold 7 ...\n",
            "Epoch    21: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    38: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    42: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Processing Seed 52, Fold 8 ...\n",
            "Epoch    20: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    33: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    37: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Processing Seed 52, Fold 9 ...\n",
            "Epoch    29: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    33: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    37: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Processing Seed 52, Fold 10 ...\n",
            "Epoch    19: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    26: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch    30: reducing learning rate of group 0 to 1.0000e-06.\n",
            "\n",
            "------------------------------\n",
            "\n",
            "Time Taken to Train 10 folds for 50 epochs : 29.52 minutes\n",
            "\n",
            "------------------------------\n",
            "\n",
            "Best Loss : {'train': 0.00019128300482407212, 'valid': 0.00017012498574331403}\n",
            "\n",
            "------------------------------\n",
            "\n",
            "Training Completed\n",
            "\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "\n",
            "Log Loss : 0.01659\n",
            "\n",
            "------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcW8xEszyoOj",
        "outputId": "fadef359-6cbd-4f13-c3d4-9fc7bc1c4f06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "def ensemble_eval_fn(model_1=None, model_2=None, names_1=None, names_2=None, dataloader=None):\n",
        "  y_pred_1 = np.zeros((num_obs_test, num_classes))\n",
        "  y_pred_2 = np.zeros((num_obs_test, num_classes))\n",
        "\n",
        "  for name in names_1:\n",
        "    Pred = torch.zeros(cfg.ts_batch_size, num_classes).to(cfg.device)\n",
        "    model_1.load_state_dict(torch.load(datapath+name))\n",
        "    model_1.eval()\n",
        "    for feat in dataloader:\n",
        "      feat = feat.to(cfg.device)\n",
        "      with torch.no_grad():\n",
        "        Prob = model_1(feat)\n",
        "      Pred = torch.cat((Pred, Prob), dim=0)\n",
        "    Pred = Pred[cfg.ts_batch_size:]\n",
        "    Pred = Pred.cpu().numpy()\n",
        "    y_pred_1 = np.add(y_pred_1, Pred)\n",
        "  y_pred_1 = np.divide(y_pred_1, len(names_1))\n",
        "\n",
        "  for name in names_2:\n",
        "    Pred = torch.zeros(cfg.ts_batch_size, num_classes).to(cfg.device)\n",
        "    model_2.load_state_dict(torch.load(datapath+name))\n",
        "    model_2.eval()\n",
        "    for feat in dataloader:\n",
        "      feat = feat.to(cfg.device)\n",
        "      with torch.no_grad():\n",
        "        Prob = model_2(feat)\n",
        "      Pred = torch.cat((Pred, Prob), dim=0)\n",
        "    Pred = Pred[cfg.ts_batch_size:]\n",
        "    Pred = Pred.cpu().numpy()\n",
        "    y_pred_2 = np.add(y_pred_2, Pred)\n",
        "  y_pred_2 = np.divide(y_pred_2, len(names_2))\n",
        "\n",
        "  y_pred = np.divide(np.add(y_pred_1, y_pred_2), 2)\n",
        "  return y_pred\n",
        "\n",
        "ensemble_y_pred = ensemble_eval_fn(Network_1, Network_2, names_1, names_2, ts_data)\n",
        "\n",
        "breaker()\n",
        "print(\"Log Loss : {:.5f}\".format(log_loss_metric(y_test, ensemble_y_pred)))\n",
        "breaker()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "------------------------------\n",
            "\n",
            "Log Loss : 0.01649\n",
            "\n",
            "------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsRqpxyzebnZ"
      },
      "source": [
        ""
      ],
      "execution_count": 16,
      "outputs": []
    }
  ]
}